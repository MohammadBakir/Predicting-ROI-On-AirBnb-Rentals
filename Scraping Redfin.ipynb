{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set chromedriver executable path. \n",
    "chromedriver = \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "#Define function to extract walkability, transit, and bike indexes\n",
    "def get_info_and_scores(url, neighborhood_list):\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    df = pd.DataFrame(columns=['neighborhood', 'walk_score_of_100','transit_score_of_100','bike_score_of_100','median_sale_price_$'])\n",
    "    print('Scraping Neighborhoods from Redfin\\n')\n",
    "    \n",
    "    for neighborhood in neighborhood_list:\n",
    "        print('Scraping ' + neighborhood)\n",
    "        redfin_url=url\n",
    "        driver.get(redfin_url)\n",
    "        \n",
    "        search_bar = driver.find_element_by_id(\"search-box-input\")\n",
    "        search_bar.send_keys(neighborhood + ', San Francisco, CA')\n",
    "        search_bar.send_keys(Keys.RETURN)\n",
    "        \n",
    "        #Wait 10 seconds for page to load\n",
    "        WebDriverWait(driver, 10).until(EC.url_changes(redfin_url))\n",
    "        \n",
    "        try:\n",
    "            #Find div containing walk-scores class and median sale price \n",
    "            content_walk = driver.find_element_by_xpath(\"//div[contains(@class, 'walk-score')]\")\n",
    "            content_med_sale_price = driver.find_element_by_xpath(\"//div[contains(@class, 'trends')]//li[5]//div//span[2]//span\")\n",
    "            \n",
    "            #convert div html to text\n",
    "            content_walk_text =content_walk.text\n",
    "            content_med_sale_text = content_med_sale_price.text\n",
    "            \n",
    "            #Extract out of 100 scores from div\n",
    "            info_scores = [int(s) for s in content_walk_text.split() if s.isdigit()]\n",
    "            info_scores = [info_scores[0],info_scores[2],info_scores[4]]\n",
    "            \n",
    "            #Add neighborhood and median sale price to info_list\n",
    "            info_scores.insert(0,neighborhood)\n",
    "            info_scores.insert(len(info_scores),content_med_sale_text)\n",
    "        \n",
    "            #add to walkability dataframe\n",
    "            df.loc[-1] = info_scores  # adding a row\n",
    "            df.index = df.index + 1  # shifting index\n",
    "            df = df.sort_index()\n",
    "        \n",
    "        except NoSuchElementException:\n",
    "            filler=[neighborhood,np.NaN,np.NaN,np.NaN,np.NaN]\n",
    "            df.loc[-1] = filler  # adding a row\n",
    "            df.index = df.index + 1  # shifting index\n",
    "            df = df.sort_index()\n",
    "    \n",
    "    print('Scraping Done!')\n",
    "            \n",
    "    return df\n",
    "\n",
    "#Define site URL for scraping\n",
    "url=\"https://www.redfin.com/\"\n",
    "\n",
    "#Define neighborhood list as available in the SF data\n",
    "neighborhood_list = ['Western Addition', 'Bernal Heights', 'Haight Ashbury', 'Mission',\\\n",
    "       'Potrero Hill', 'Civic Center / Van Ness', 'Castro','Upper Market',\\\n",
    "       'Inner Sunset', 'South of Market', 'Noe Valley', 'Outer Richmond',\\\n",
    "       'Presidio Heights', 'Nob Hill', 'Ocean View Terrace', 'Pacific Heights',\\\n",
    "       'Financial District', 'Twin Peaks', 'Russian Hill', 'Outer Sunset',\\\n",
    "       'North Beach', 'Glen Park', 'Marina Distric', 'Inner Richmond',\\\n",
    "       'Excelsior', 'Seacliff', 'Chinatown', 'Bayview', 'Diamond Heights',\\\n",
    "       'West of Twin Peaks', 'Outer Mission', 'Parkside', 'Lakeshore',\\\n",
    "       'Crocker Amazon', 'Golden Gate Park', 'Visitacion Valley','Presidio Heights']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Neighborhoods from Redfin\n",
      "\n",
      "Scraping Western Addition\n",
      "Scraping Bernal Heights\n",
      "Scraping Haight Ashbury\n",
      "Scraping Mission\n",
      "Scraping Potrero Hill\n",
      "Scraping Civic Center / Van Ness\n",
      "Scraping Castro\n",
      "Scraping Upper Market\n",
      "Scraping Inner Sunset\n",
      "Scraping South of Market\n",
      "Scraping Noe Valley\n",
      "Scraping Outer Richmond\n",
      "Scraping Presidio Heights\n",
      "Scraping Nob Hill\n",
      "Scraping Ocean View Terrace\n",
      "Scraping Pacific Heights\n",
      "Scraping Financial District\n",
      "Scraping Twin Peaks\n",
      "Scraping Russian Hill\n",
      "Scraping Outer Sunset\n",
      "Scraping North Beach\n",
      "Scraping Glen Park\n",
      "Scraping Marina Distric\n",
      "Scraping Inner Richmond\n",
      "Scraping Excelsior\n",
      "Scraping Seacliff\n",
      "Scraping Chinatown\n",
      "Scraping Bayview\n",
      "Scraping Diamond Heights\n",
      "Scraping West of Twin Peaks\n",
      "Scraping Outer Mission\n",
      "Scraping Parkside\n",
      "Scraping Lakeshore\n",
      "Scraping Crocker Amazon\n",
      "Scraping Golden Gate Park\n",
      "Scraping Visitacion Valley\n",
      "Scraping Presidio Heights\n",
      "Scraping Done!\n"
     ]
    }
   ],
   "source": [
    "df = get_info_and_scores(url, neighborhood_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data to csv file\n",
    "df.to_csv('..\\Data\\San Francisco Data\\sf_walk_and_sale_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
